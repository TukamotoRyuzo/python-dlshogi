# python-dlshogi

## 使い方

- python-dlshogiをpythonのモジュールとして使う
  - `pip install --no-cache-dir -e .`
- csa棋譜リストから条件以外の棋譜を取り除いて棋譜を集出する
  - `python utils/filter_csa.py <棋譜リストがあるディレクトリのパス>`
- 抽出した棋譜をシャッフル&訓練データとテストデータに分割する
  - `python utils/make_kifu_list.py <棋譜リストがあるディレクトリのパス> <保存するファイル名のプレフィクス>`
  - 実行するとカレントディレクトリに、以下の2つのファイルができあがる
    - <保存するファイル名のプレフィクス>_train.txt
    - <保存するファイル名のプレフィクス>_test.txt
- 学習実行
  - `python train_policy.py <トレーニング用棋譜リスト> <テスト用棋譜リスト> --eval_interval <テストデータを評価する間隔>`

## Google colaboratory上で実行するための環境構築

- はやく[`Google colaboratory`](https://colab.research.google.com/)にアクセスするんだ
- `GPU`をアサインしよう(これ忘れてるとGPU使えないぞ)
  - 画面上部のメニュー ランタイム -> ランタイムのタイプを変更 -> ノートブックの設定 を開く
  - ハードウェアアクセラレータに GPU を選択し、 保存 する

### Deep Leaning用のライブラリを入れよう

- `Keras`は以下のコマンドを実行（しなくてもいいかも）[[1]](https://qiita.com/tomo_makes/items/f70fe48c428d3a61e131) [[2]](https://qiita.com/stakemura/items/1761be70a06fa8ee853f#chainer)
  - `!pip install keras`
- `chainer`ならこう

``` shell
!apt-get install -y -qq libcusparse8.0 libnvrtc8.0 libnvtoolsext1
!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so
!pip install cupy-cuda80==4.0.0b4
!pip install chainer==4.0.0b4
```

### Google Drive をマウントしよう

- Colaboratoryは大容量データのアップロードが苦手なので、そういうのはGoogle Driveに入れとこう

- 以下のコマンドを実行する [[3]](https://qiita.com/uni-3/items/201aaa2708260cc790b8)

``` python
from google.colab import drive
drive.mount('/content/gdrive')
```

- gdriveのMy Drive以下にdriveのルートディレクトリがマウントされている
- 嘘だと思うなら以下のコマンドを実行
  - `!ls ./gdrive/'My Drive'`

### `github`からリポジトリをクローンしよう

- `!git clone https://github.com/TukamotoRyuzo/python-dlshogi.git`

### 棋譜を用意しよう

- Google Driveに棋譜入れとこう
- zip圧縮されてない状態で入れておこう
- zip圧縮されていたらunzipしよう
  - `!unzip ./gdrive/'My Drive'/wdoor2016.zip`
  - とても時間かかるよ

### python-dlshogiを使ってみよう

- ディレクトリ移動
  - `cd python-dlshogi`
- 必要なもの入れとく
  - `!pip install --no-cache-dir -e .`
  - `!pip install python-shogi`
- 棋譜のフィルタリング
  - `!python utils/filter_csa.py "../gdrive/My Drive/wdoor2016/2016"`
  - __これcolaboratory上だと時間かかりすぎるから諦めた方が良い__
- 棋譜リスト作ろう
  - `!python utils/make_kifu_list.py "../gdrive/My Drive/wdoor2016/2016" kifulist`
- 学習を回そう
  - 少なめの学習データで実行
    - `!python train_policy.py kifulist_train_1000.txt kifulist_test_100.txt --eval_interval 100`
  - 普通に実行
    - `!python train_policy.py kifulist_train.txt kifulist_test.txt --eval_interval 10000`

## ディレクトリ構成

- bat
  - いろんな探索方法で遊ばせてみるためのコマンド

- model
  - モデルの重みファイル(policyとvalue)

- pydlshogi
  - read_kifu.py
    - 棋譜の読み込み
  - features.py
    - 局面 -> 入力特徴
    - 指し手 -> 出力ラベル
  - common.py
    - 定数定義とか盤面回転処理とか
  - network
    - policy.py
      - 方策ネットワーク(policy network)の実装
  - player
    - いろんな探索方法で探索してみるプレイヤー実装
  - uct
    - uct探索実装
  - usi
    - playerをusiエンジン化する実装

- utils
  - 教師のお掃除とかログ出力とかもろもろ
  - filter_csa.py
    - csa棋譜リストから条件以外の棋譜を取り除いて棋譜を集出する
  - make_kifu_list.py
    - 抽出した棋譜をシャッフル&訓練データとテストデータに分割する
- train_policy.py
  - 方策ネットワークを学習する

- 7.1 ソースコードの構成
  - 上記の通りの構成

- 7.2 モジュールインストール
  - `setup.py`を作成し、`pip install --no^cache-dir -e .`で別のスクリプトからimportできるようにする

- 7.3 方策ネットワークの構成
  - カラー画像をRGBの3チャンネルに分けて入力する
  - 出力層では、各ラベルに分類される確率が出力される

  - 入力
    - 将棋においては、局面を、駒毎にチャンネルを分けて入力する
      - 各チャンネルは、駒があるかないかの2値画像
      - 成駒（6種類）もチャンネルを分ける（金と同一チャネルにはしない）
      - 持ち駒は、最大持てる枚数分だけチャンネルを用意する
        - 金なら4チャンネル、飛車なら2チャンネル、歩なら18チャンネル
        - 持っていればすべて1、持っていなければすべて0
      - 先手、後手それぞれにチャンネルを持つ
  - 出力
    - 座標に一つのラベルを割り当てる
    - そこに動く可能性を出力する
    - ただし移動元の情報が必要
    - 移動先と方向が決まれば移動元が決まる
    - 持ち駒は持ち駒の種類にラベルを割り当てる
    - 出力ラベル数は
      - 移動先 81種類
      - 移動元
        - 方向  10種類 
        - 駒うち 7種類
        - 成り 10種類
      - 合計 81 * 27 = 2187
  - NNの構成
    - 13層のCNN
    - プーリング層は使わない
    - 出力層は、1 * 1 フィルターの畳み込み * 27種類（出力ラベルの数）
    - inceptionでも使っているらしい
- 7.4 方策ネットワークの実装
  - chainer出の実装が書いてある
    - 104は入力チャンネルの数
    - 出力層はbiasあり
    - chainerではNCHW(ミニバッチ、チャンネル、y座標、x座標)
    - TensorFlowではNHWC形式(チャンネルが最後)
- 7.5 訓練データの用意
  - 用意してね
  - 7.5.1 棋譜のクリーニング
    - これはルール違反したりするソフトが混ざっているため
    - 弱い棋譜はゴミ箱にポイする
  - 7.5.2 訓練データとテストデータに分割  
    - 訓練用とテスト用に分けるスクリプトの紹介
      - .csaファイルパスのリストを作るっぽい
- 7.6 python-shogi
  - pythonで将棋の局面構造を定義したもの
  - BoardとMoveが定義されている
- 7.7 共通処理の実装  
  - 棋譜の読み込み
    - .csa（CSAという将棋の棋譜の形式）を読み込める
    - 盤面のbb、occupied bb、持ち駒（collections counter）、指し手、現局面の手番が勝ったかどうかをリストとして保持 
    - 局面から入力特徴を作成  
    - 各チャンネルごとに、numpyの配列で0/1を入れていく
    - 後手番の局面は、180度回転して入力特徴を作るんだってサ
  - 指し手から出力ラベルを作成
- 7.8 学習処理の実装
  - optimizerはSGD
  - optimizerの状態ってあるんだね
    - epoch数とか、AdaGradだとg2のことだと思う
  - pickle?
    - 漬物 
    - インスタンス化したクラスをメモリでなくディスクに置いとけるものかと思っている
    - プログラム終了後もロードできるそうな
  - ミニバッチ作成  
  - 学習ループ開始
    - Trainer?
- 7.9 学習実行
  - 実行しよう

## モンテカルロ木探索

### ハッシュ

``` python
def hash_to_index(hash):
    return ((hash & 0xffffffff) ^ ((hash >> 32) & 0xffffffff)) & (UCT_HASH_SIZE - 1)
```

- hashの上位32bit と下位32bitをxorして、その中から UCT_HASH_SIZE - 1 ビットだけ使う
  - UST_HASH_SIZEは 0000 1000 0000 0000 みたいな値なので UCT_HASH_SIZE は 0000 0111 1111 1111見たいな値になる
- 衝突対策は?
  - UCT_HASH_SIZEはあまり大きな値にできない
    - メモリが足りないので
  - だから違う局面でも同じインデックスになることが多々ある
  - 局面が同じかどうか調べるため、以下の情報をハッシュエントリーの中に含めておく
    - ハッシュ値
    - 手番
    - 手数
    - 使用中フラグ
    - これらが全く同じなら同じ局面であると判断でき、安心してハッシュエントリーの情報を使うことができる
- search_empty_index
  - ハッシュテーブルから使っていないところ探すメソッド  
  - テーブル走査して、使用フラグがfalseのところを探すだけ
- find_same_hash_index
  - ハッシュテーブルから、`登録されている`局面のハッシュ値に対応するindexを返す
    - ただしindexの場所が未使用なら配列の最大値を返し、未使用であることを知らせる 
    - indexの場所が使用中でも、さっき言った衝突対策を実施してからindexを返す
- ハッシュフル
  - 局面をハッシュテーブルに登録するとき、違う局面が登録されていたら、いまのindexから+1して空いている場所をさがす
  - でも空きが見つからないこともある
  - だから登録されている局面の数を記録しておき、最大値の何割かを使ったら検知できるようにしておこう
    - 検知してどうするんかはわからないけど
- 合流
  - UCTでは親ノードからみて、子ノードの勝ち数、負け数を見てこのードを選ぶ可判断する
    - しかし、合流があるので、別の親からその子ノードの勝ち数、負け数がカウントされたりするとうまく機能しない
    - そこで親ノードにこのードの勝ち数、負けすを持たせることで`合流`に対応した！
  - 千日手問題
    - 対応しない

### モンテカルト木探索の実装

- プレイアウト  
  - ルートノードから末端ノードまでたどることをプレイアウトという
    - 本来プレイアウトは、ゲーム終了まで指し続けることを言うので、ちょっと意味が違う
    - 温度パラメータというのは`ボルツマン分布`に使う
- go 
- 古いハッシュの削除
  - まず全部の局面を`未使用`状態にします
  - そして、現局面から展開済みの局面を再帰的に`仕様中`状態にします

- ルートノード展開
  - 候補手が一つなら展開しないですぐ指す
  - まあ一長一短あるが
- プレイアウト繰り返し
  - プレイアウトを、設定した回数に達するまで繰り返す
  - uct_search出1回繰り返します
- 指して選択
  - 手数が10手までの局面はばらけさせるためにランダム要素をいｓれるよ
    - 勝ちに徹するならここも勝率が最大の手を選んだ方が良いんでしょうね
  - 10手目以降はランダム要素なし。勝率が最大の手選ぶよ
- 探索結果表示
  - usi出認識できる形式にしたらうれしい

- 勝率評価値変換
  - centi pawnにt直すというだけの話

- ノード展開
  - 割り当てずみチェック
    - すでにハッシュテーブルに登録されているかを調べるよ 
    - 割り当ててなかったら割り当てるよ
      - 展開したノードは割り当てる！
  - 候補手で展開
    - すべての合法手で展開するよ
      - legal_movesをchild_moveに
      - legal_movesの数をchild_numに
      - このードはまだ未展開のため、child_indexはNOT_EXPAND。
      - child_move_countは０に
        - 子ノードの`訪問回数`を入れるよ
      - child_winも0
        - 子ノードの勝率の合計らしいが勝率の合計とは？？
  - 評価
    - 価値ネットワークを使う場合、通常のUCTとは違い、展開したらかならず評価します
    - 現ノードを価値ネットワークで評価しちゃおう
      - 中では価値もポリシーもどっちも評価してる。
  - 最後にindexを変えそう
    - ここまで`ノード展開`。忘れそうになる。
- 評価
  - 局面 -> 特徴 -> ResNet -> 価値&指し手確率分布
  - 指し手確率分布
    - 合法手でフィルター
    - 正規化 
      - ようするに確率の合計を1にします
      - 温度パラーメタ適用  
        - この辺は、わからない。私にとってはブラックボックス。
- ノード情報更新
  - 評価したから記録しとこう
- 探索
  - uct_searchで実装しちゃいます
  - uct_searchの戻り値は、渡した局面の、相手から見た勝率
  - ルートノード展開してから、uct_search開始
    - 詰みチェック
    - ハッシュテーブルから読み込む
    - 選択
      - UCB値が最大となる手
    - 着手
    - 子ノードが未展開なら展開する
      - expand_node
      - ここで評価
    - バックアップ
      - 勝率の合計をたす
      - このーどの勝率の合計にも
      - 訪問回数持たす
- 選択
  - PUCTアルゴリズム
  - Q(st,a)+U(st,a)
  - Q(st,a)
    - 価値ネットワークの勝率の平均
    - ええ。。価値ネットワークの勝率って一回測れば後は不変なんじゃ
    - と思ったけど、子ノードが展開済みだったら、さらに1手深く読んだ結果得られた勝率を足すのか。
    - つまり展開されるたびにresultは変わるから、平均を取る意味はあるんだな
  - +U(st,a)
    - 方策ネットワークの予測した着手確率 * まだ訪問回数がすくない手を優先する
- 探索打ち切り
  - 1番勝ち数の多い指してが、2番目に勝ち数の多い指して + 残りプレイアウト回数となったとき打ち切る